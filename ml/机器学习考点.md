# 决策树

- $ID3$算法、$C4.5$算法、$CART$算法基本原理，见统计学习方法
- 为什么$ID3$算法倾向于选择取值多的特征，$C4.5$算法倾向于取值少的特征？
  - $ID3$算法用信息增益，当特征取值多时，$H(D, A)$较小，信息增益较大；
  - $C4.5$算法用信息增益比，即对信息增益添加了一个惩罚系数$IV(A)$（基于熵公式），当特征取值多时，特征A的熵越小，对应的倒数越大，即$IV(A)$越大。
  - 注意：$C4.5$在进行特征选择时，首先利用信息增益进行特征筛选，然后利用信息增益比在剩下的特征中进行筛选。
- 预剪枝和后剪枝区别
- 决策树的损失函数：每个叶子节点的熵值和
- 剪枝和正则系数$\alpha$的确定（见统计学习方法）。

# 集成学习

## bagging和boosting区别

## 随机森林

- 随机性体现：样本随机抽样、特征随机抽样
- 特征选择：
  - 步骤1：构建$N$个基学习器，计算损失函数$F$
  - 步骤2：对特征$X_1$，随机添加噪声，重新计算损失函数$F_1$
  - 步骤3：对其他特征重复步骤2
  - 步骤4：针对每个特征$F_n-F$排序，值越大，表明该特征越重要。
  - 步骤5：确定特征筛选阈值，比如删除掉步骤4中尾部20%的特征。
  - 重复步骤1-5，直到筛选出符合要求的特征数量。

# XGBoost基本原理

参考博客：https://zhuanlan.zhihu.com/p/92837676

### 构造目标函数

Boosting的预测值是相加的，大致思路如下：原始数据：

| $ID$ | $X$ | $Y$ |
| :----: | :---: | :---: |
|   1    |  12   |  22   |
|   2    |  34   |  12   |
|   3    |  22   |  32   |
|   4    |  14   |  15   |
|   5    |  11   |   6   |

第一轮：基于弱学习器$f_1$，以$X$为输入，得到预测值$\hat Y_1$，并得到与$Y_0$，即真实值的残差$Residual_1$，很明显，弱学习器$f_1$的预测能力较差，预测结果的残差较大。

| $ID$ | $X$ | $Y_1$ | $Residual_1$ |
| :----: | :---: | :-----: | :------------: |
|   1    |  12   |   18    |       4        |
|   2    |  34   |   14    |       -2       |
|   3    |  22   |   35    |       -3       |
|   4    |  14   |   18    |       -3       |
|   5    |  11   |   11    |       -5       |

第二轮：基于弱学习$f_2$，以$X$为输入，得到预测值$\hat Y_2$，并得到与第一轮预测结果的残差$Residual_1$的残差$Residual_2$

| $ID$ | $X$ | $Y_2$ | $Residual_2$ |
| :----: | :---: | :-----: | :------------: |
|   1    |  12   |    4    |       0        |
|   2    |  34   |  -1.5   |      -0.5      |
|   3    |  22   |   -2    |       -1       |
|   4    |  14   |   -4    |       1        |
|   5    |  11   |  -5.5   |      0.5       |

(如果第二轮残差仍然较大，照此思路继续训练弱学习器)

假设到第二轮，我们得到的残差值已经足够小，则停止训练，得到的最终预测值为$\hat Y_1+\hat Y_2$：

| $ID$ | $X$ | $Y$ | 预测值$\hat Y_1+\hat Y_2$ | 残差 |
| :----: | :---: | :---: | :-------------------------: | :--: |
|   1    |  12   |  22   |             22              |  0   |
|   2    |  34   |  12   |            12.5             | -0.5 |
|   3    |  22   |  32   |             33              |  -1  |
|   4    |  14   |  15   |             14              |  1   |
|   5    |  11   |   6   |             5.5             | 0.5  |

因此，最终的预测值 = 模型1预测值 + 模型2预测值 + 模型3预测值 + ....公式为：
$$
\hat Y_i = \sum_{k=1}^Kf_k(X_i),\qquad f_k \in F,即一共K个学习器
$$
从训练流程来说，Boosting是串行的，而Bagging是并行的。

- 首先，构造初始的目标函数如下：
  $$
  Obj_K = \sum_{i=1}^{n}l(y_i, \hat y_i)+\sum_{k=1}^K\Omega(f_k) 
  $$

  其中：$\sum_{i=1}^{n}l(y_i, \hat y_i)$为损失函数，$\sum_{k=1}^K\Omega(f_k)$为模型复杂度。(模型复杂度包括但不限于叶子节点数量、树深度、叶子节点值等)  


- 主要问题：已知前$K-1$棵树，如何构建第$K$棵树？
  对于样本$i$,第一棵树预测值为$ \hat y_i^{(1)}$, 第二棵树的预测值为$\hat y_i^{(2)}$，...，第$K-1$棵树的预测值为$\hat y_i^{(K-1)}$。

  给定$X_i$：
  $$
  \hat y_i^{(0)} = 0\\
  \hat y_i^{(1)} = f_1(X_i) = \hat y_i^{(0)} + f_1(X_i) \\
  \hat y_i^{(2)} = f_1(X_i) + f_2(X_i)= \hat y_i^{(1)} + f_2(X_i)\\
  ...\\
  \hat y_i^{(K)} = f_{K-1}(X_i) + f_K(X_i)= \hat y_i^{(K-1)} + f_K(X_i)
  $$
  得到第$K$棵树的预测值为：
  $$
  \hat y_i^{(K)} = \sum_{k=1}^{K-1}f_k(X_i)+f_K(X_i)
  $$

- 基于上述$\hat y_i^{(K)}$的表达式，重写损失函数：
  $$
  \begin{equation}
  \begin{aligned}
  Obj_K &= \sum_{i=1}^{n}l(y_i, \hat y_i)+\sum_{k=1}^K\Omega(f_k)\\
  &=\sum_{i=1}^{n}l(y_i, \hat y_i^{(K)})+\sum_{k=1}^K\Omega(f_k)\\
  &=\sum_{i=1}^{n}l(y_i, \hat y_i^{(K-1)}+f_k(X_i))+\sum_{i=1}^{K-1}\Omega(f_i) + \Omega(f_K)\\
  \end{aligned}
  \end{equation}
  $$

其中：$\hat y_i^{(K-1)}$和$\sum_{i=1}^{K-1}\Omega(f_i)$都是常数项。

需要使用$\hat y_i^{(K-1)}$ 估计 $\hat y _i^{(K)}$，因此保留，$\sum_{i=1}^{K-1}\Omega(f_i)$可以删除。
	
所以，当训练第$K$棵树时，我们的目标是：

$$
Minimize: \sum_{i=1}^{n}l(y_i, \hat y_i^{(K-1)}+f_k(X_i))+\Omega(f_K)
$$
以上是训练第$K$棵树时我们需要优化的目标函数，但仅仅是一个框架，下面将解释：目标函数如何近似？

## 如何近似目标函数

根据泰勒表达式，若函数$ f(x) $在包含$x_0$的某个闭区间$[a,b]$上具有$n$阶导数，且在开区间$(a,b)$上具有$n+1$阶导数，则对[闭区间$[a,b]$上任意一点$x$，成立下式：

$$
f(x)=\frac{f(x_0)}{0!}+\frac{f^{'}(x_0)}{1!}(x-x_0)+\frac{f^{''}(x_0)}{2!}(x-x_0)^2+...+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)
$$

，即
$$
f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}
$$

根据前文推导，假设树的结构已知，得到目标函数$ Obj_k$：
$$
\sum_{i=1}^{n}l(y_i, \hat y_i^{(K-1)}+f_k(X_i))+\Omega(f_K)
$$

假设$ f(x) = l(y_i, \hat y_i^{(K-1)}),  \Delta x = f_K(X_i)$ ,则 $f(x+ \Delta x) = l(y_i, \hat y_i^{(K-1)}+f_k(X_i)) $


因此，利用泰勒表达式二阶展开目标函数：

$$
Obj_k = \sum_{i=1}^n[l(y_i, \hat y_i^{(K-1)}) + \frac{\partial{l(y_i, \hat y_i^{(K-1)})}}{\partial y_i^{(K-1)}}*f_K(X_i)+\frac{1}{2}\frac{\partial^{2}{l(y_i, \hat y_i^{(K-1)})}}{\partial {y_i^{(K-1)}}^2}*f_K^2(X_i)+\Omega(f_K)\\
$$
其中：$\sum_{i=1}^n[l(y_i, \hat y_i^{(K-1)})$为常数，不参与优化，可以删除。$ \frac{\partial{l(y_i, \hat y_i^{(K-1)})}}{\partial y_i^{(K-1)}}$是常数，令其为$g_i$，$\frac{\partial^{2}{l(y_i, \hat y_i^{(K-1)})}}{\partial {y_i^{(K-1)}}^2}$也为常数，令其为$h_i$, 损失函数可以简化为：
$$
Minimize:Obj_K = \sum_{i=1}^{n}[g_i*f_K(X_i)+\frac{1}{2}h_i*f_K^2(X_i)]+\Omega(f_K)
$$
当训练第K棵树时，$ \{g_i,h_i\}$已知。


### 将树结构引入到目标函数

上文构造出了目标函数的新形式，接下来的目标时：将$ f_K(X_i)和\Omega(f_K)$参数化。

假设一棵树，叶子节点值定义为$w$,则下图中叶子节点的值分别为$ (w_1,w_2,w_3)$；定义样本$X_i$的位置为$q(X_i)$，则下图中，叶节点1位置为1，叶节点2位置为2，叶节点3位置为3，参数化$ f_K(X_i)=w_{q(X_i)}$,定义$I_j=\{i|q(X_i)=j\}$，即所有落在位置$j$的样本。

```mermaid
graph TD
	A-->B;
	A-->C(C:叶节点3);
	B-->D(D:叶节点1);
	B-->E(E:叶子节点2);
```


定义树的复杂度：$\Omega(f_K) = \gamma *T+\frac{1}{2}\lambda*\sum_{j=1}^Tw_j^2 $，其中$T$表示叶子节点数量，$w_j^2$表示叶子节点的值，$\gamma 和 \lambda$为超参数。

假设树的形状已知，则目标函数可改写为：
$$
\begin{equation}
\begin{aligned}
Obj_K &= \sum_{i=1}^{n}[g_i*f_K(X_i)+\frac{1}{2}h_i*f_K^2(X_i)]+\Omega(f_K)\\
&=\sum_{i=1}^n[g_i*w_{q(X_i)}+\frac{1}{2}h_i*w^2_{q(X_i)}]+\gamma*T+\frac{1}{2}*\lambda*\sum_{j=1}^Tw_j^2\\
&=\sum_{j=1}^T[(\sum_{i\in I_j}g_i)*w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)*w_j^2]+\gamma *T\\
& (其中，\sum_{i\in I_j}g_i为常数，令其为G_j,\sum_{i\in I_j}h_i 也为常数，令其为H_j)\\
&=\sum_{j=1}^T[G_j*w_j+\frac{1}{2}*(H_j+\lambda )*w_j^2]+\gamma*T
\end{aligned}
\end{equation}
$$

很明显，上式为一个一元二次方程$ (ax^2+bx+c=0)$，在$T$确定的情况下，最优解为$ -\frac{b}{2a}$，则上式最优解为$-\frac{G_j}{H_j+\lambda}$，故得到目标函数的最优值：
$$
Obj_K^* = -\frac{1}{2}*\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma*T
$$

### 寻找最优的Split

前文的推导均假设树的结构已知，并得到了最优的目标函数值。如果树的结构未知，如何做？

决策树在划分时，基于信息增益/信息增益比/基尼系数，寻找最优的split。

同样，根据我们计算出来的最优目标函数值，在划分特征时，尽量选择$$Gain=Obj^*_{Knew}-Obj^*_{Kold}$$最大的特征进行划分。
$$
\begin{equation}
\begin{aligned}
Gain&=Obj^*_{Knew}-Obj^*_{Kold}\\
&=\frac{1}{2}*[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma
\end{aligned}
\end{equation}
$$

### 总结
1. XGB的优点？
- 支持多种损失函数。其构造目标函数时将Loss损失函数置入常数项，使其可以支持多种损失函数
- 支持并行化，对特征值提前进行遍历，并保存为内存中的block结构

2. XGB分裂原则？
- 目标函数Obj在分裂前后的差值

3. XGB在拟合什么？
- XGB没有拟合目标，而是依据规则对树进行分裂，在接近真实值y的同时控制树的复杂度，使整体的模型具有泛化性

4. 列块并行学习细节
- 在树生成过程中，最耗时的一个步骤就是在每次寻找最佳分裂点时都需要对特征的值进行排序。而 XGBoost 在训练之前会根据特征对数据进行排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量

  
- 通过顺序访问排序后的块遍历样本特征的特征值，方便进行切分点的查找。此外分块存储后多个特征之间互不干涉，可以使用多线程同时对不同的特征进行切分点查找，即特征的并行化处理。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 XGBoost 能够实现分布式或者多线程计算的原因

- 注意：这是一种按列进行分割数据集的方法，其他算法中存在按行（按样本）分割数据集进行计算的情况

5. XGB 对缺失值的处理方式

- 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。
- 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。
如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。

6. XGB特征重要性 feature_importance 的计算逻辑
- weight:指某特征在所有树中被作为分裂节点的次数
- gain：指某特征在作为节点分裂时带来的目标函数gain优化的平均值
- cover：指特征被分裂时涵盖的样本树总和 / 被分裂的次数

7. 控制过拟合的参数
- max_depth : 树的深度
- min_child_weight : 叶子节点的最小权重
- gamma/min_split_loss : 节点分裂后的gain提升最小阈值，若小于阈值则不分裂
- subsample/colsample_bytree ： 列采样，样本采样，使得树形状更多样化
- alpha/lambda ： alpha 为 L1 正则，lambda 为 L2 正则

8. XGBoost和LightGBM的区别
- 树生长策略：XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。

- 分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法


9.  为什么使用泰勒二阶展开
- 损失函数的梯度总是不容易求得，所以用泰勒公式去近似的拟合
- 同时二阶信息本身就能让梯度收敛更快更准确